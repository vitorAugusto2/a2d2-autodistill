# ‚úÇÔ∏è AUTODISTILL
## 1. INTRODUCAO
- O Autodistill usa modelos b√°sicos grandes e mais lentos para treinar modelos supervisionados pequenos e mais r√°pidos. Usando autodistill, voc√™ pode passar de imagens n√£o rotuladas para infer√™ncia em um modelo personalizado executado na borda, sem interven√ß√£o humana no meio
- [Rotular automaticamente conjuntos de dados](https://docs.autodistill.com/)
  
## 2. CONCEITOS BASICOS
- Para usar autodistill, voc√™ insere dados n√£o rotulados em um modelo base que usa uma ontologia para rotular um conjunto de dados que √© usado para treinar um modelo de destino que gera um modelo destilado ajustado para executar uma tarefa espec√≠fica.
- Autodistill define v√°rias primitivas b√°sicas:
    - **Tarefa** - Uma tarefa define o que um modelo de destino ir√° prever. A tarefa de cada componente (modelo base, ontologia e modelo de destino) de um autodistillpipeline deve corresponder para que sejam compat√≠veis entre si. Atualmente, a detec√ß√£o de objetos e a segmenta√ß√£o de inst√¢ncias s√£o suportadas por meio da detectiontarefa. classificationo suporte ser√° adicionado em breve.
    - **Modelo Base** - Um Modelo Base √© um modelo b√°sico grande que sabe muito sobre muita coisa. Os modelos b√°sicos costumam ser multimodais e podem executar muitas tarefas. Eles s√£o grandes, lentos e caros. Exemplos de modelos b√°sicos s√£o GroundedSAM e a pr√≥xima variante multimodal do GPT-4. Usamos um modelo base (junto com dados de entrada n√£o rotulados e uma ontologia) para criar um conjunto de dados.
    - **Ontologia** - uma Ontologia define como seu Modelo Base √© solicitado, o que seu Conjunto de Dados descrever√° e o que seu Modelo Alvo ir√° prever. Uma ontologia simples √© aquela CaptionOntologyque solicita um modelo base com legendas de texto e os mapeia para nomes de classes. Outras Ontologias podem, por exemplo, usar um vetor CLIP ou imagens de exemplo em vez de uma legenda de texto; uma ontologia em Computa√ß√£o √© um modelo de dados que representa um conjunto de conceitos dentro de um dom√≠nio e os relacionamentos entre estes.
    - **Conjunto de dados** - um conjunto de dados √© um conjunto de dados rotulados automaticamente que pode ser usado para treinar um modelo de destino. √â a sa√≠da gerada por um Modelo Base.
    - **Modelo de destino** - um modelo de destino √© um modelo supervisionado que consome um conjunto de dados e gera um modelo destilado que est√° pronto para implanta√ß√£o. Os modelos de destino geralmente s√£o pequenos, r√°pidos e ajustados para executar muito bem uma tarefa espec√≠fica (mas n√£o generalizam muito al√©m das informa√ß√µes descritas em seu conjunto de dados). Exemplos de modelos de destino s√£o YOLOv8 e DETR.
    - **Modelo Destilado** ‚Äì um Modelo Destilado √© o resultado final do autodistillprocesso; √© um conjunto de pesos ajustados para sua tarefa que pode ser implantado para obter previs√µes.
### 2.1. Conceitos Principais
- Um modelo base , que √© usado para rotular dados automaticamente. Os exemplos incluem Grounding DINO, Grounded SAM, DETIC CLIP. 
- Um modelo de destino , que √© treinado nos dados rotulados automaticamente. Os exemplos incluem YOLOv5, [**YOLOv8**](https://github.com/autodistill/autodistill-yolov8) e DETR.

## 3. MODELO BASE UTILIZADO
- **Grounding DINO**
    - [Grounding DINO](https://github.com/autodistill/autodistill-grounding-dino): √© um detector de objetos de disparo zero eficaz que pode identificar uma ampla variedade de objetos, desde carros at√© capas de discos de vinil.

# üîé ULTRALYTICS YOLOv8
## 1. INTRODUCAO 
- modelo de deteccao de objetos e segmentacao de imagens em tempo real. Aprendizagem profunda e visao computacional. Alem disso, consegue classificar os objetos.
- 5 modelos [YOLOv8](https://docs.ultralytics.com/pt/models/yolov8/): YOLOv8n, YOLOv8s, YOLOv8m, YOLOv8l e YOLOv8x

## 2. CONFIGURACOES
### 2.1. Modos
- YOLO Os modelos podem ser utilizados em diferentes modos, dependendo do problema espec√≠fico que est√°s a tentar 
resolver. Estes modos incluem:
    - [**train** ](https://docs.ultralytics.com/pt/modes/train/): Para treinar um modelo YOLOv8 num conjunto de dados personalizado.
    - [**val**](https://docs.ultralytics.com/pt/modes/val/): Para validar um modelo YOLOv8 depois de ter sido treinado.
    - [**predict**](https://docs.ultralytics.com/pt/modes/predict/): Para fazer previs√µes utilizando um modelo YOLOv8 treinado em novas imagens ou v√≠deos.
    - [**export**](https://docs.ultralytics.com/pt/modes/export/): Para exportar um modelo YOLOv8 para um formato que possa ser usado para implanta√ß√£o.
    - [**track**](https://docs.ultralytics.com/pt/modes/track/): Para seguir objectos em tempo real utilizando um modelo YOLOv8 .
    - [**benchmark**](https://docs.ultralytics.com/pt/modes/benchmark/): Para aferir a velocidade e a precis√£o das exporta√ß√µes YOLOv8 (ONNX, TensorRT, etc.).
### 2.2. Tarefas
- YOLO podem ser utilizados para uma variedade de tarefas, incluindo dete√ß√£o, segmenta√ß√£o, classifica√ß√£o e pose. Estas 
tarefas diferem no tipo de resultados que produzem e no problema espec√≠fico que foram concebidos para resolver.
    - [**detect**](https://docs.ultralytics.com/pt/tasks/detect/): Para identificar e localizar objectos ou regi√µes de interesse numa imagem ou v√≠deo. O resultado de um 
    detetor de objectos √© um conjunto de caixas delimitadoras que envolvem os objectos na imagem, juntamente com 
    etiquetas de classe e pontua√ß√µes de confian√ßa para cada caixa.
    - [**segment**](https://docs.ultralytics.com/pt/tasks/segment/): Para dividir uma imagem ou v√≠deo em regi√µes ou pixels que correspondem a diferentes objectos ou 
    classes. O resultado de um modelo de segmenta√ß√£o de inst√¢ncias √© um conjunto de m√°scaras ou contornos que delineia 
    cada objeto na imagem, juntamente com etiquetas de classe e pontua√ß√µes de confian√ßa para cada objeto.
    - [**classify**](https://docs.ultralytics.com/pt/tasks/classify/): Para prever a etiqueta da classe de uma imagem de entrada. O resultado de um classificador de 
    imagens √© uma etiqueta de classe √∫nica e uma pontua√ß√£o de confian√ßa.
    - [**pose**](https://docs.ultralytics.com/pt/tasks/pose/): Para identificar objectos e estimar os seus pontos-chave numa imagem ou v√≠deo.
    - [**OBB**](https://docs.ultralytics.com/pt/tasks/obb/): Caixas delimitadoras orientadas (ou seja, rodadas) adequadas para imagens de sat√©lite ou m√©dicas.
### 2.3. Argumentos
As defini√ß√µes de treino para os modelos YOLO englobam v√°rios [hiperpar√¢metros e configura√ß√µes](https://docs.ultralytics.com/pt/modes/train/#train-settings) utilizados durante o processo de treino. Estas defini√ß√µes influenciam o desempenho, a velocidade e a precis√£o do modelo. As principais defini√ß√µes de treino incluem o tamanho do lote, a taxa de aprendizagem, o momento e a diminui√ß√£o do peso. Al√©m disso, a escolha do optimizador, a fun√ß√£o de perda e a composi√ß√£o do conjunto de dados de treino podem ter impacto no processo de treino. O ajuste cuidadoso e a experimenta√ß√£o com estas defini√ß√µes s√£o cruciais para otimizar o desempenho.

## 3. METRICAS DE TREINAMENTO
![image](https://github.com/vitorAugusto2/tcc-a2d2/assets/131685750/efb27566-e7c7-42ed-aaf8-c876da5e7e23)

- **Epoch**: ciclos de treinamento ou numero de vezes que iteramos 
- **GPU_mem**: memoria da GPU
- **box_loss**: perda de caixa -> perda da localizacao
- **cls_loss**: perda de classificacao -> especifica se o objeto esta classificado corretamente ou nao
- **dfl_loss**: perda focal distribuida -> detectores de objetos sem ancora 
- **instances**: numero de instancias -> numeros de rotulos no lote atual
- **size**: tamanho da entrada -> tamanho da imagem (padrao 640)
- **class**: quantidade de classe
- **images**: quantidade de imagens
- **instances**: quantidade de instancias
- **Boxp**: precisao da caixa
- **R**: valor de recuperacao
- **mAP50**: metrica de precisao

## 4. METRICAS DE DESEMPENHO
[Tabelas com o desempenho de deteccao](https://docs.ultralytics.com/pt/models/yolov8/#supported-tasks-and-modes)

| Modelo  | Tamanho (pix√©is)  | mAPval | Velocidade CPU ONNX (ms)  | Velocidade A100 TensorRT (ms) | Params (M) | FLOPs (B)  |
|---------|-------------------|--------|---------------------------|-------------------------------|------------|------------|
| YOLOv8n | 640               | 37.3   | 80.4                      | 0.99                          | 3.2        | 8.7        |
| YOLOv8s | 640               | 44.9   | 128.4                     | 1.20                          | 11.2       | 28.6       |
| YOLOv8m | 640               | 50.2   | 234.7                     | 1.83                          | 25.9       | 78.9       |
| YOLOv8l | 640               | 52.9   | 375.2                     | 2.39                          | 43.7       | 165.2      |
| YOLOv8x | 640               | 53.9   | 479.1                     | 3.53                          | 68.2       | 257.8      |

- Os modelos Detect, Segment e Pose s√£o pr√©-treinados no conjunto de dados **COCO**, enquanto os modelos classificacao s√£o pr√©-treinados no conjunto de dados **ImageNet**. 
- **tamanho(pixels)**: resolucao das imagens (padrao=640)
- **Deteccao (**COCO**)/ m√©dia de precis√£o**: mAPval50-95 -> 50 a 95 indica a m√©dia da precis√£o m√©dia em diferentes n√≠veis de confian√ßa de detec√ß√£o, que v√£o de 50% a 95% de confian√ßa
- **Velocidade CPU ONNX (ms)**: tempo m√©dio de infer√™ncia em milissegundos para os modelos executando em uma CPU utilizando a estrutura ONNX 
- **Velocidade A100 TensorRT (ms)**: tempo m√©dio de infer√™ncia em milissegundos para os modelos executando em um A100 TensorRT
- **params (M)**: n√∫mero de par√¢metros (em milh√µes) que o modelo possui
- **FLOPs (B)**: n√∫mero de opera√ß√µes de ponto flutuante (em bilh√µes) que o modelo realiza durante a infer√™ncia.


## 5. GRAFICO DE TREINAMENTO
![Resultado do grafico de treinamento](https://github.com/vitorAugusto2/tcc-a2d2-autodistill-yolo/blob/main/runs/detect/test2/yolov8n_200i_10e_train/results.png)
### 5.1. Metricas de treinamento (conjunto de treinamento)
- **train/box_loss**: perda (loss) associada √† localiza√ß√£o (bounding box)
- **train/cls_loss**: perda associada √† classifica√ß√£o dos objetos (ou seja, atribuir a categoria correta aos objetos detectados) 
- **train/dfl_loss**: "defocus loss", objetos fora de foco em imagens
### 5.2. Metricas de validacao (conjunto de validacao)
- **val/box_loss**: perda associada √† localiza√ß√£o dos objetos
- **val/cls_loss**: perda associada √† classifica√ß√£o dos objetos
- **val/dfl_loss**: perda associada √† localiza√ß√£o dos objetos
### 5.3. Metricas de avaliacao (acuracia)
- **metrics/precision(B)**: Precis√£o (Precision) para a classe 'B'. Isso geralmente se refere √† propor√ß√£o de verdadeiros positivos (TP) sobre a soma de TP e falsos positivos (FP).
- **metrics/recall(B)**: Revoca√ß√£o (Recall) para a classe 'B'. Isso geralmente se refere √† propor√ß√£o de TP sobre a soma de TP e falsos negativos (FN).
- **metrics/mAP50(B)**: Precisao m√©dia com threshold de IoU de 0.5 para a classe 'B'.
- **metrics/mAP50-95(B)**: Precisao m√©dia com threshold variando de 0.5 a 0.95 para a classe 'B'.
